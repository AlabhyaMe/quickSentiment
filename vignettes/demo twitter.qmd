```{r}
library(quickSentiment)
library(readr)
```

```{r}
#Import tweet data
tweets <- read_csv("tweets.csv")
#Since value ranges from -2 to 2, make it factor of P and N
#we might also include neutral

tweets$sentiment <- ifelse(tweets$Avg >0,'P','N')
```

```{r}
#pre_process is the ffirst major function of this package. it does all the cleaning works like
#-removing brackets, lowercase, lemmitization etc for you
tweets$cleaned <- pre_process(tweets$Tweet)
```

```{r}
#now feed in the pipeline
#you need to send it in datafram
#add the column that is cleaned and sentiments that you are predicting
#vectorization methods like bag of words, tf, tfidf can be called inside the function
#you can assign the prediction model like logistic regression, random forest and xgboost

#1. Logistic Regression
result_logit <- pipeline(df =tweets,
                   text_column_name = "cleaned", #name of the pre processed column
                   sentiment_column_name = "sentiment",  #name of the sentiment column
                   vect_method = "bow",  #bow, tf or tfidf
                   model_name ="logit")  #logit, rf or xgb

```

```{r}
#now make a prediction
#this document will predict in the same file, but you can use completely new file

tweets$prediction_logit <- prediction(
  pipeline_object = result_logit,  #this is the object that was returned previously,
  df = tweets, #dataframe name
  text_column = "cleaned"  #columns used to make prediction, note the vectorization and ml methods are borrowed from above
)
```

```{r}
#2. Random Forest
result_rf<- pipeline(df =tweets,
                         text_column_name = "cleaned", #name of the pre processed column
                         sentiment_column_name = "sentiment",  #name of the sentiment column
                         vect_method = "bow",  #bow, tf or tfidf
                         model_name ="rf")  #logit, rf or xgb  ## we changed the model here

tweets$prediction_rf <- prediction(
  pipeline_object = result_rf,  #this is the object that was returned previously, #we change the object here for new model
  df = tweets, #dataframe name
  text_column = "cleaned"  #columns used to make prediction, note the vectorization and ml methods are borrowed from above
)
```

```{r}
#3. XGBoost
result_xgb<- pipeline(df =tweets,
                     text_column_name = "cleaned", #name of the pre processed column
                     sentiment_column_name = "sentiment",  #name of the sentiment column
                     vect_method = "bow",  #bow, tf or tfidf
                     model_name ="xgb")  #logit, rf or xgb  we changed the model here

tweets$prediction_xgb <- prediction(
  pipeline_object = result_xgb,  #this is the object that was returned previously,#we change the object here for new model
  df = tweets, #dataframe name
  text_column = "cleaned"  #columns used to make prediction, note the vectorization and ml methods are borrowed from above
)

```

```{r}
# let us look at results from different model. not, we used BOW vectorization for all the model
table(tweets$sentiment,tweets$prediction_logit)
accuracy <- table(tweets$sentiment, tweets$prediction_logit)
sum(diag(accuracy)) / sum(accuracy)
```

```{r}
table(tweets$sentiment,tweets$prediction_rf)
accuracy <- table(tweets$sentiment, tweets$prediction_rf)
sum(diag(accuracy)) / sum(accuracy)
```

```{r}
table(tweets$sentiment,tweets$prediction_xgb)
accuracy <- table(tweets$sentiment, tweets$prediction_xgb)
sum(diag(accuracy)) / sum(accuracy)

#Random forest gets over 91% accuracy
```
